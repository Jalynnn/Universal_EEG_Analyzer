{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install mne\n",
    "# pip install autoreject\n",
    "# pip install pyxdf\n",
    "# pip install numpy\n",
    "# pip install pandas\n",
    "# pip install scipy\n",
    "# pip install matplotlib\n",
    "# pip install pyxdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import mne\n",
    "from mne.preprocessing import ICA\n",
    "from autoreject import AutoReject\n",
    "import pyxdf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compress Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore runtime warnings for clean output (Came with the SK code)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "# \"Effective window size : 1.024 (s)\" woudl print 5000x (J added)\n",
    "mne.set_log_level('WARNING')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Obtain XDF and CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_file(participant):\n",
    "    # Build the folder path for the given participant\n",
    "    folder = os.path.join(\"Raw\", participant)\n",
    "    \n",
    "    # Find all .xdf files in that folder\n",
    "    files = glob.glob(os.path.join(folder, \"*.xdf\"))\n",
    "    \n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .xdf file found in {folder}\")\n",
    "    \n",
    "    # Return the first file found (or adjust if you expect more than one)\n",
    "    return files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Global Variables (Determine your variables here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participants are expected to have .xdf files and .csv files\n",
    "participants = [\"P01\"] #, \"P07\", \"P15\"]\n",
    "\n",
    "# Uses LSL Values (The CSV File)\n",
    "conditions = {\n",
    "    \"LL\": {\"cond_start\": 42, \"cond_end\": 12},\n",
    "    \"LH\": {\"cond_start\": 43, \"cond_end\": 13},\n",
    "    \"HL\": {\"cond_start\": 44, \"cond_end\": 14},\n",
    "    \"HH\": {\"cond_start\": 46, \"cond_end\": 16},\n",
    "}\n",
    "\n",
    "# Define regions and their corresponding channels\n",
    "regions = {\n",
    "    'Frontal': ['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC3', 'FC5', 'ACC_X', 'ACC_Y', 'ACC_Z'],\n",
    "    'Central': ['C3'],\n",
    "    'Parietal': ['P3', 'P4', 'PZ'],\n",
    "    'Occipital': ['O1', 'O2'],\n",
    "}\n",
    "\n",
    "# What channels do you want to work with?\n",
    "channel_names = [\"Fz\", \"Cz\", \"Pz\", \"Acc1\", \"Acc2\", \"Acc3\"]\n",
    "channels_to_drop = ['F7', 'F9', 'FC5', 'FC1', 'C3']\n",
    "\n",
    "# Freqency analysis? Define ranges here\n",
    "bands = {\n",
    "    'Delta': (0.5, 4),\n",
    "    'Theta': (4, 7),\n",
    "    'Alpha': (8, 13),\n",
    "    'Beta': (14, 20),\n",
    "    'Gamma': (20, 100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# Analysis Type\n",
    "Frequency = 1\n",
    "ERP_option = 0\n",
    "\n",
    "# EEG or Acc\n",
    "eeg_channels = ['Fz', 'Cz', 'Pz']\n",
    "acc_channels = ['Acc1', 'Acc2', 'Acc3']\n",
    "\n",
    "# Downsample value\n",
    "downsample_value = 250\n",
    "bandpass_low = 0.1\n",
    "bandpass_high = 30\n",
    "\n",
    "# Choose Preprocessing Options\n",
    "# ICA_option = 1\n",
    "\n",
    "# Yes or No: Epochs?\n",
    "# Epochs = 1\n",
    "# Interval = 2 # In seconds\n",
    "\n",
    "# Only create tables or run entire analysis\n",
    "# analysis = 0\n",
    "# table = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading Data & Channel Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: channel_info = load_channel_info(channel_mat, channel_names)\n",
    "\n",
    "def load_channel_info(channel_mat, channel_names):\n",
    "    print(\"Step 2.1: Loading channel.mat file for channel names...\")\n",
    "\n",
    "    # Loads the .mat file with the 6 channels\n",
    "    mat_data = loadmat(channel_mat)\n",
    "    print(f\"Load Channel Info: Channel.mat Data: {mat_data}\")\n",
    "\n",
    "    # Hard code channel names\n",
    "    # Extracts the channel names from a matlab file and saves in a [] - I did not want since it extracted all 11 channels\n",
    "    # channel_names = [str(mat_data['Channel']['Name'][0][i][0]) for i in range(mat_data['Channel']['Name'].shape[1])]\n",
    "    channel_names = channel_names\n",
    "    print(f\"Load Channel Info: Hardcoded Channel Names: {channel_names}\")\n",
    "\n",
    "    # Extract channel locations and compresses into x,y,z for further analysis\n",
    "    channel_locs = np.array([mat_data['Channel']['Loc'][0][i][:3] for i in range(mat_data['Channel']['Loc'].shape[1])]).squeeze().T\n",
    "\n",
    "    # Creates a dictionary for the channel names with their associated locations\n",
    "    return {'names': channel_names, 'locs': channel_locs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: raw = load_set(set_file, channel_info)\n",
    "\n",
    "def load_set(set_file, channel_info):\n",
    "    print(\"Step 2.2: Loading .xdf file...\")\n",
    "\n",
    "    # Load the .xdf file\n",
    "    streams, _ = pyxdf.load_xdf(set_file)\n",
    "\n",
    "    # Find the EEG stream\n",
    "    eeg_stream = next((s for s in streams if s['info']['type'][0].lower() == 'eeg'), None)\n",
    "    if eeg_stream is None:\n",
    "        raise ValueError(\"No EEG stream found in the XDF file.\")\n",
    "\n",
    "    # Extract data and sampling frequency\n",
    "    data = np.array(eeg_stream['time_series']).T\n",
    "    try:\n",
    "        sfreq = float(eeg_stream['info']['sample_rate'][0])\n",
    "    except:\n",
    "        sfreq = downsample_value\n",
    "    print(f'Load Set: sfreq: {sfreq}')\n",
    "\n",
    "    # Extract channel names from the stream\n",
    "    try:\n",
    "        ch_names = [chan['label'][0] for chan in eeg_stream['info']['desc'][0]['channels'][0]['channel']]\n",
    "    except:\n",
    "        ch_names = [f\"Ch{i+1}\" for i in range(data.shape[0])]\n",
    "    # Provides the .mat channels\n",
    "    print(f'Load Set: Channel names: {ch_names}')\n",
    "\n",
    "    # Create the MNE Raw object\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=['eeg'] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info)\n",
    "\n",
    "    # Replace channel names using the .mat file info\n",
    "    ch_names_from_mat = channel_info['names']\n",
    "    print(f\"Load Set: Original hardcoded channel names: {ch_names_from_mat}\")\n",
    "    if len(raw.ch_names) != len(ch_names_from_mat):\n",
    "        # Drop channels not in the .mat file\n",
    "        if channels_to_drop:\n",
    "            print(f\"Load Set: Dropping channels: {channels_to_drop}\")\n",
    "            raw.drop_channels(channels_to_drop)\n",
    "\n",
    "    # Rename channels and set montage\n",
    "    raw.rename_channels({raw.ch_names[i]: ch_names_from_mat[i] for i in range(len(raw.ch_names))})\n",
    "    montage = mne.channels.make_dig_montage(ch_pos=dict(zip(ch_names_from_mat, channel_info['locs'].T)))\n",
    "\n",
    "    # Set channel types (EEG vs. accelerometer)\n",
    "    # We need to set up acc and eeg differently\n",
    "    channel_types = {ch: 'eeg' for ch in eeg_channels}\n",
    "    channel_types.update({ch: 'misc' for ch in acc_channels})\n",
    "    raw.set_channel_types(channel_types)\n",
    "\n",
    "    raw.set_montage(montage)\n",
    "\n",
    "    # Downsample and apply a bandpass filter\n",
    "    print(f\"Step 2.2.1: Downsampling to {downsample_value} Hz...\")\n",
    "    raw.resample(downsample_value)\n",
    "    print(f\"Step 2.2.2: Applying bandpass filter ({bandpass_low} Hz to {bandpass_high} Hz)...\")\n",
    "    raw.filter(l_freq=bandpass_low, h_freq=bandpass_high, picks='eeg')\n",
    "\n",
    "    print(\"Load Set: EEG data successfully loaded and preprocessed.\")\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Event Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: raw_training, raw_test = extract_event_windows(raw, marker_df, cond_start, cond_end)\n",
    "\n",
    "def extract_event_windows(raw, marker_df, cond_start, cond_end, output_loc, cond):\n",
    "    print(\"\\nStep 3.1: Extracting event windows for 'training' and 'test' periods...\")\n",
    "\n",
    "    if 'time' not in marker_df.columns or 'value' not in marker_df.columns:\n",
    "        raise KeyError(\"The CSV file must contain 'time' and 'value' columns.\")\n",
    "    marker_df['value'] = pd.to_numeric(marker_df['value'], errors='coerce').dropna().astype(int)\n",
    "    marker_df['time'] = pd.to_numeric(marker_df['time'], errors='coerce').dropna()\n",
    "    # Had to reset the index for removed rows\n",
    "    marker_df = marker_df.dropna(subset=['time', 'value']).reset_index(drop=True)\n",
    "\n",
    "    events = marker_df['value']\n",
    "    print(f\"Extract Event Windows: LSL Values: {marker_df['value']}\")\n",
    "    times = marker_df['time']\n",
    "\n",
    "    # Get indices for the specific marker values\n",
    "    event_42_indices = [i for i, event in enumerate(events) if event == cond_start]\n",
    "    print(f\"Extract Event Windows: Indices of First Condition Start: {events.iloc[event_42_indices[0]]}\")\n",
    "    event_12_indices = [i for i, event in enumerate(events) if event == cond_end]\n",
    "\n",
    "    # Changed the first mention of 6 from a 1 - Rereading this idk what this meant\n",
    "    if len(event_42_indices) < 7 or len(event_12_indices) < 7:\n",
    "        print(\"Extract Event Windows: Insufficient markers for training or test period.\")\n",
    "        print(f'Extract Event Windows: Length of 42/46 indices: {len(event_42_indices)}')\n",
    "        print(f'Extract Event Windows: Length of 12/16 indices: {len(event_12_indices)}')\n",
    "        return None, None\n",
    "\n",
    "    # Use .iloc?\n",
    "    # training_start = times.iloc[event_42_indices[0]]\n",
    "    # print(f'Actual training start: {training_start}')\n",
    "    # training_end = times.iloc[event_12_indices[5]]\n",
    "    # print(f'Actual training end: {training_end}')\n",
    "    # test_start = times.iloc[event_42_indices[-1]]\n",
    "    # test_end = times.iloc[event_12_indices[-1]]\n",
    "    training_start = times[event_42_indices[0]]  # First \"42\"\n",
    "    print(f'Extract Event Windows: Actual training start: {training_start}')\n",
    "    training_end = times[event_12_indices[5]]  # 6th \"12\" # Jay: For some reason this was the last one... changed from 5 to 4 !!\n",
    "    print(f'Extract Event Windows: Actual training end: {training_end}')\n",
    "    test_start = times[event_42_indices[-1]]  # Last \"42\"\n",
    "    test_end = times[event_12_indices[-1]]  # Last \"12\"\n",
    "\n",
    "    # Crop raw data for training and test periods\n",
    "    # Jay: some clarification on what is happening here\n",
    "    # Jay: Crop works with seconds. So taking the start sample / frequency provides seconds\n",
    "    # Jay: Crops then copies the data from that start and end section\n",
    "    # Jay: AHH we don't want this. Time is already in seconds\n",
    "    # raw_training = raw.copy().crop(tmin=training_start / sfreq, tmax=training_end / sfreq) if training_start < training_end else None\n",
    "    # raw_test = raw.copy().crop(tmin=test_start / sfreq, tmax=test_end / sfreq) if test_start < test_end else None\n",
    "    raw_training = raw.copy().crop(tmin=training_start, tmax=training_end) if training_start < training_end else None\n",
    "    raw_test = raw.copy().crop(tmin=test_start, tmax=test_end) if test_start < test_end else None\n",
    "\n",
    "    # Save cropped data\n",
    "    if raw_training:\n",
    "        training_file = f\"{output_loc}/fif/{cond}_training_data.fif\"\n",
    "        raw_training.save(training_file, overwrite=True)\n",
    "        print(f\"Extract Event Windows: Training data saved to '{training_file}'\")\n",
    "    else:\n",
    "        print(\"Extract Event Windows: Invalid training period duration.\")\n",
    "\n",
    "    if raw_test:\n",
    "        test_file = f\"{output_loc}/fif/{cond}_test_data.fif\"\n",
    "        raw_test.save(test_file, overwrite=True)\n",
    "        print(f\"Extract Event Windows: Test data saved to '{test_file}'\")\n",
    "    else:\n",
    "        print(\"Extract Event Windows: Invalid test period duration.\")\n",
    "\n",
    "    # Log durations\n",
    "    if raw_training:\n",
    "        print(f\"Extract Event Windows: Training data duration: {(training_end - training_start)} seconds\")\n",
    "    if raw_test:\n",
    "        print(f\"Extract Event Windows: Test data duration: {(test_end - test_start)} seconds\")\n",
    "\n",
    "    # Save condition timestamps\n",
    "    condition_timestamps = {\n",
    "        'training_start': training_start,\n",
    "        'training_end': training_end,\n",
    "        'test_start': test_start,\n",
    "        'test_end': test_end,\n",
    "    }\n",
    "\n",
    "    pd.DataFrame([condition_timestamps]).to_csv(f\"{output_loc}/{cond}_condition_timestamps.csv\", index=False)\n",
    "    print(f\"Extract Event Windows: Condition timestamps saved to '{output_loc}/{cond}_condition_timestamps.csv'.\")\n",
    "\n",
    "    # Prepare and save trial data\n",
    "    trials = []\n",
    "\n",
    "    # Loop through each pair of 42 and 12\n",
    "    for i, start_index in enumerate(event_42_indices):\n",
    "        # Ensure there's a corresponding \"12\" index\n",
    "        if i < len(event_12_indices):\n",
    "            end_index = event_12_indices[i]\n",
    "\n",
    "            # Determine condition based on whether it's the last 42/12 pair\n",
    "            condition = 'train' if i < len(event_42_indices) - 1 else 'test'\n",
    "\n",
    "            # Add data for both the start (42) and end (12) events\n",
    "            # trials.append({'event_type': events.iloc[start_index], 'sample_idx': start_index, 'time': times.iloc[start_index], 'condition': condition})\n",
    "            # trials.append({'event_type': events.iloc[end_index], 'sample_idx': end_index, 'time': times.iloc[end_index], 'condition': condition})\n",
    "\n",
    "            # Use .iloc?\n",
    "            trials.append({'event_type': events[start_index], 'sample_idx': start_index, 'time': times[start_index], 'condition': condition})\n",
    "            trials.append({'event_type': events[end_index], 'sample_idx': end_index, 'time': times[end_index], 'condition': condition})\n",
    "\n",
    "    if len(event_42_indices) > len(event_12_indices):\n",
    "        last_start_index = event_42_indices[-1]\n",
    "\n",
    "        # Use .iloc?\n",
    "        # trials.append({'event_type': events.iloc[last_start_index], 'sample_idx': last_start_index, 'time': times.iloc[last_start_index], 'condition': 'test'})\n",
    "        trials.append({'event_type': events[last_start_index], 'sample_idx': last_start_index, 'time': times[last_start_index], 'condition': 'test'})\n",
    "\n",
    "    # Create a DataFrame and save to a CSV\n",
    "    trials_df = pd.DataFrame(trials)\n",
    "    trials_df.to_csv(f\"{output_loc}/{cond}_trials.csv\", index=False)\n",
    "\n",
    "    print(f\"Extract Event Windows: Trials saved to '{output_loc}/{cond}_trials.csv'.\")\n",
    "\n",
    "    return raw_training, raw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Preprocessing Functions (Baseline, Autoreject, ICA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: raw_training = apply_baseline_correction(raw_training, marker_df, condition_type)\n",
    "# Process Pipeline: raw_test = apply_baseline_correction(raw_test, marker_df, condition_type)\n",
    "\n",
    "def apply_baseline_correction(raw, marker_df, condition_type, output_loc, cond):\n",
    "    print(\"\\nStep 4.1: Applying baseline correction...\")\n",
    "\n",
    "    sfreq = raw.info['sfreq']\n",
    "\n",
    "    print(\"Apply Baseline Correction: Detecting rest intervals from markers...\")\n",
    "\n",
    "    rest_start_event = marker_df[marker_df['value'] == 200].index.tolist()\n",
    "    rest_end_event = marker_df[marker_df['value'] == 210].index.tolist()\n",
    "\n",
    "    # Pass the index not the timestamp at the index\n",
    "    rest_start = rest_start_event[0] if rest_start_event else None\n",
    "    rest_end = rest_end_event[0] if rest_end_event else None\n",
    "\n",
    "    # Verify the rest interval\n",
    "    if rest_start is not None and rest_end is not None and rest_end > rest_start:\n",
    "        print(f\"Apply Baseline Correction: Using detected rest interval for baseline correction: Start={rest_start}, End={rest_end}\")\n",
    "        try:\n",
    "            raw.apply_function(\n",
    "                lambda x: x - np.mean(x[:, rest_start:rest_end], axis=-1)[:, None],\n",
    "                picks='eeg', channel_wise=False\n",
    "            )\n",
    "            print(\"Apply Baseline Correction: Baseline correction using rest interval applied successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Apply Baseline Correction: Error applying baseline correction with rest interval: {e}\")\n",
    "    else:\n",
    "        if rest_start is None or rest_end is None:\n",
    "            print(\"Apply Baseline Correction: Warning: Rest interval markers not detected.\")\n",
    "        elif rest_end <= rest_start:\n",
    "            print(\"Apply Baseline Correction: Warning: Detected rest interval is invalid (end occurs before or at start).\")\n",
    "\n",
    "        print(\"Apply Baseline Correction: Falling back to default 15s baseline...\")\n",
    "        # First 15 seconds of the recording\n",
    "        baseline_start = int(max(0, (raw.times[0] + 15) * sfreq))\n",
    "        # Start of the recording\n",
    "        baseline_end = int(max(0, raw.times[0] * sfreq))\n",
    "\n",
    "        if baseline_end > baseline_start:\n",
    "            print(f\"Apply Baseline Correction: Using default baseline: Start={baseline_start}, End={baseline_end}\")\n",
    "            try:\n",
    "                raw.apply_function(\n",
    "                    lambda x: x - np.mean(x[:, baseline_start:baseline_end], axis=-1),\n",
    "                    picks='eeg', channel_wise=False\n",
    "                )\n",
    "                print(\"Apply Baseline Correction: Baseline correction using default interval applied successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Apply Baseline Correction: Error applying baseline correction with default interval: {e}\")\n",
    "        else:\n",
    "            print(\"Apply Baseline Correction: Error: Default baseline interval is invalid. Skipping baseline correction.\")\n",
    "\n",
    "    # Save the baseline-corrected file\n",
    "    print(f\"Saving baseline-corrected file to '{output_loc}/fif/{cond}_D_bc_{condition_type}.fif'...\")\n",
    "    raw.save(f\"{output_loc}/fif/{cond}_D_bc_{condition_type}.fif\", overwrite=True)\n",
    "\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: raw_training, ica_training = apply_ica(raw_training, fixed_channels, condition_type)\n",
    "# Process Pipeline: raw_test, ica_test = apply_ica(raw_test, fixed_channels, condition_type)\n",
    "\n",
    "def apply_ica(raw, fixed_channels, condition_type, output_loc, cond):\n",
    "    print(\"\\nStep 4.2: Applying ICA...\")\n",
    "\n",
    "    # again, just going to hard code the value since we don't expect the number of eeg channels to change\n",
    "    n_components = 3\n",
    "    ica = ICA(n_components=n_components, random_state=97, max_iter=800)\n",
    "\n",
    "    # Select EEG channels only for ICA\n",
    "    picks = mne.pick_types(raw.info, eeg=True, exclude='bads')\n",
    "    print(f\"Apply ICA: EEG Picks: {picks}\")\n",
    "    ica.fit(raw, picks=picks)\n",
    "\n",
    "    # Handle absence of EOG channels\n",
    "    # Use channels from fixed_channels for EOG if available\n",
    "    eog_indices, eog_scores = [], []\n",
    "    eog_channels = [ch for ch in fixed_channels if ch in raw.ch_names]\n",
    "    if eog_channels:\n",
    "        eog_indices, eog_scores = ica.find_bads_eog(raw, ch_name=eog_channels)\n",
    "        print(f\"Apply ICA: Found EOG components: {eog_indices}\")\n",
    "    else:\n",
    "        print(\"Apply ICA: No EOG channels found; skipping EOG artifact detection.\")\n",
    "        eog_indices = []\n",
    "\n",
    "    # Detect ECG artifacts if possible\n",
    "    try:\n",
    "        ecg_indices, ecg_scores = ica.find_bads_ecg(raw, method='correlation')\n",
    "        print(f\"Apply ICA: Found ECG components: {ecg_indices}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Apply ICA: ECG artifact detection skipped: {e}\")\n",
    "        ecg_indices = []\n",
    "\n",
    "    # Mark bad components for exclusion\n",
    "    ica.exclude = eog_indices + ecg_indices\n",
    "    print(f\"Apply ICA: Excluded components: {ica.exclude}\")\n",
    "\n",
    "    # Apply ICA to the raw data\n",
    "    raw = ica.apply(raw)\n",
    "    print(f\"Apply ICA: ICA applied with {n_components} components. Excluded {len(ica.exclude)} components.\")\n",
    "\n",
    "    # Save the cleaned EEG files\n",
    "    raw.save(f\"{output_loc}/fif/{cond}_D_cleaned_{condition_type}.fif\", overwrite=True)\n",
    "    print(f\"Apply ICA: Cleaned EEG file saved as '{output_loc}/fif/{cond}_D_cleaned_{condition_type}.fif'.\")\n",
    "\n",
    "    return raw, ica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: raw_training = apply_autoreject(raw_training)\n",
    "# Process Pipeline: raw_test = apply_autoreject(raw_test)\n",
    "\n",
    "def apply_autoreject(raw):\n",
    "    print(\"\\nStep 4.3: Applying AutoReject (Repair Only)...\")\n",
    "\n",
    "    # WE can change the duration here between 1 and 2 seconds\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=2.0, preload=True)\n",
    "\n",
    "    # Try different consensus levels\n",
    "    consensus_values = [1.0, 0.1, 0.5]\n",
    "    for consensus in consensus_values:\n",
    "        try:\n",
    "            print(f\"Apply AutoReject: Trying consensus={consensus}...\")\n",
    "            ar = AutoReject(\n",
    "                # Max interpolation options\n",
    "                n_interpolate=[2, 5, len(raw.ch_names)],\n",
    "                # Consensus threshold\n",
    "                consensus=[consensus],\n",
    "                thresh_method='bayesian_optimization',\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "            # Fit AutoReject\n",
    "            ar.fit(epochs)\n",
    "            epochs_clean = ar.transform(epochs, return_log=False)\n",
    "\n",
    "            # Reconstruct cleaned data into raw format\n",
    "            raw_clean = epochs_clean.get_data().reshape(len(raw.ch_names), -1)\n",
    "            raw_clean = mne.io.RawArray(raw_clean, raw.info)\n",
    "\n",
    "            print(f\"Apply AutoReject: Success with consensus={consensus}!\")\n",
    "            print(f\"Apply AutoReject: Channels after AutoReject: {raw_clean.ch_names}\")\n",
    "            return raw_clean\n",
    "        except Exception as e:\n",
    "            print(f\"Apply AutoReject: Failed with consensus={consensus}: {e}\")\n",
    "\n",
    "    print(\"Apply AutoReject: AutoReject failed. Returning original raw data.\")\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: epochs_training = epoch_data(raw_training, \"train\", duration=1.0, overlap=0.0)\n",
    "# Process Pipeline: epochs_test = epoch_data(raw_test, \"test\", duration=1.0, overlap=0.0)\n",
    "\n",
    "def epoch_data(raw, condition_label, output_loc, cond, duration=1.0, overlap=0.0):\n",
    "    print(\"\\nStep 5.1: Epoching data...\")\n",
    "\n",
    "    start = 0\n",
    "    stop = raw.n_times / raw.info['sfreq']\n",
    "    temp = raw.info['sfreq'] # J\n",
    "    print(f'Epoch Data: Raw n times: {raw.n_times}')\n",
    "    print(f'Epoch Data: Raw info sfreq: {temp}') # J\n",
    "    print(f\"Epoch Data: Raw data range: Start={start}, Stop={stop}, Duration={duration}, Overlap={overlap}\")\n",
    "\n",
    "    try:\n",
    "        if overlap >= duration:\n",
    "            raise ValueError(f\"Overlap must be >=0 but < duration ({duration}), got {overlap}\")\n",
    "\n",
    "        events = mne.make_fixed_length_events(\n",
    "            raw, id=1, start=start, stop=stop, duration=duration, overlap=overlap\n",
    "        )\n",
    "        print(f\"Epoch Data: Generated {len(events)} fixed-length events.\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Epoch Data: Error generating events: {e}\")\n",
    "        return None\n",
    "\n",
    "    if len(events) == 0:\n",
    "        print(\"Epoch Data: No fixed-length events created.\")\n",
    "        return None\n",
    "\n",
    "    epochs = mne.Epochs(\n",
    "        raw, events, tmin=0, tmax=duration, baseline=None, detrend=1, preload=True\n",
    "    )\n",
    "\n",
    "    # Save trial information\n",
    "    trial_data = []\n",
    "    for event in events:\n",
    "        trial_info = {\n",
    "            # Convert to seconds\n",
    "            'start': event[0] / raw.info['sfreq'],\n",
    "            'end': (event[0] + int(raw.info['sfreq'] * duration)) / raw.info['sfreq'],\n",
    "            'samples': int(raw.info['sfreq'] * duration),\n",
    "            'type': event[2]\n",
    "        }\n",
    "        trial_data.append(trial_info)\n",
    "        # trl.append(trial_info)\n",
    "\n",
    "    # Save trial data to a CSV file\n",
    "    trial_df = pd.DataFrame(trial_data)\n",
    "    filename = f\"{output_loc}/{cond}_epoch_trl_{condition_label}.csv\"\n",
    "    trial_df.to_csv(filename, index=False)\n",
    "    print(f\"Epoch Data: Epoch trial data saved to '{filename}'.\")\n",
    "\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Analysis Functions (FFT, Band Power, PSD, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: compute_and_save_band_power(epochs_training, \"train\")\n",
    "# Process Pipeline: compute_and_save_band_power(epochs_test, \"test\")\n",
    "\n",
    "def compute_and_save_band_power(epochs, condition_name, output_loc, cond):\n",
    "    print(f\"\\nStep 6.1: Computing band power for {condition_name}...\")\n",
    "\n",
    "    # Now a global variable\n",
    "    # bands = {\n",
    "    #     'Delta': (0.5, 4),\n",
    "    #     'Theta': (4, 7),\n",
    "    #     'Alpha': (8, 13),\n",
    "    #     'Beta': (14, 20),\n",
    "    #     'Gamma': (20, 100)\n",
    "    # }\n",
    "\n",
    "    band_power_results = []\n",
    "\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        for ch_idx, channel_name in enumerate(epochs.ch_names):\n",
    "\n",
    "            psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "                epoch_data[ch_idx], sfreq=epochs.info['sfreq'], fmin=0.1, fmax=30.0, n_per_seg=128\n",
    "            )\n",
    "\n",
    "            band_power = {}\n",
    "            for band, (fmin, fmax) in bands.items():\n",
    "                band_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "                band_power[f'{band}_power'] = psd[band_mask].mean() if band_mask.any() else 0\n",
    "\n",
    "            band_power['Epoch'] = epoch_idx + 1\n",
    "            band_power['Channel'] = channel_name\n",
    "            band_power_results.append(band_power)\n",
    "\n",
    "    band_power_df = pd.DataFrame(band_power_results)\n",
    "    output_file = f\"{output_loc}/{cond}_band_power_{condition_name}.csv\"\n",
    "    band_power_df.to_csv(output_file, index=False)\n",
    "    print(f\"Band power results saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Pipeline: compute_psd_and_ratios(raw_training, \"train\")\n",
    "# Process Pipeline: compute_psd_and_ratios(raw_test, \"test\")\n",
    "\n",
    "def compute_psd_and_ratios(raw, phase, participant, output_loc, cond):\n",
    "    print(\"\\nStep 6.2: Computing PSD and ratios for analysis...\")\n",
    "\n",
    "    psd_results = []\n",
    "    ratio_results = []\n",
    "\n",
    "    # bands = {\n",
    "    #     'delta': (0.5, 4),\n",
    "    #     'theta': (4, 7),\n",
    "    #     'alpha': (8, 13),\n",
    "    #     'beta': (14, 20),\n",
    "    #     'gamma': (20, 100)\n",
    "    # }\n",
    "\n",
    "    try:\n",
    "        # Compute PSD for each channel\n",
    "        for channel_idx, channel_name in enumerate(raw.ch_names):\n",
    "            # Extract channel data\n",
    "            psd_data = raw.get_data(picks=[channel_idx])\n",
    "            psd_values, freqs = mne.time_frequency.psd_array_welch(\n",
    "                psd_data, sfreq=raw.info['sfreq'], fmin=0.1, fmax=30.0, n_per_seg=int(4 * raw.info['sfreq'])\n",
    "            )\n",
    "            # PSD values are returned in a nested array\n",
    "            psd_values = psd_values[0]\n",
    "\n",
    "            # Calculate mean power for each band\n",
    "            psd_band_values = {'Channel': channel_name}\n",
    "            for band, (fmin, fmax) in bands.items():\n",
    "                band_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "                psd_band_values[f'psd_{band}'] = np.mean(psd_values[band_mask]) if np.any(band_mask) else np.nan\n",
    "\n",
    "            psd_results.append(psd_band_values)\n",
    "\n",
    "        # Save PSD results to a CSV file\n",
    "        psd_df = pd.DataFrame(psd_results)\n",
    "        psd_df.to_csv(f'{output_loc}/{cond}_psd_results_{phase}.csv', index=False)\n",
    "        print(f\"Compute PSD & Ratios: PSD results saved to '{output_loc}/{cond}_psd_results_{phase}.csv'.\")\n",
    "\n",
    "        if psd_df.empty:\n",
    "            print(\"Compute PSD & Ratios: Error: PSD results are empty. Cannot compute ratios.\")\n",
    "            return\n",
    "\n",
    "        # Retrieve values for specific channels\n",
    "        pz_alpha = psd_df.loc[psd_df['Channel'] == 'Pz', 'psd_Alpha'].values\n",
    "        fz_theta = psd_df.loc[psd_df['Channel'] == 'Fz', 'psd_Theta'].values\n",
    "\n",
    "        # Handle cases where the channel values are missing or zero\n",
    "        if len(pz_alpha) > 0:\n",
    "            pz_alpha = pz_alpha[0]\n",
    "        else:\n",
    "            print(\"Compute PSD & Ratios: Warning: Missing value for 'Pz Alpha'. Setting to NaN.\")\n",
    "            pz_alpha = np.nan\n",
    "\n",
    "        if len(fz_theta) > 0 and fz_theta[0] != 0:\n",
    "            fz_theta = fz_theta[0]\n",
    "        else:\n",
    "            print(\"Compute PSD & Ratios: Warning: 'Fz Theta' is zero or invalid. Setting to NaN.\")\n",
    "            fz_theta = np.nan\n",
    "\n",
    "        # Calculate alpha/theta ratios\n",
    "        alpha_theta_ratio = pz_alpha / fz_theta if not np.isnan(fz_theta) else np.nan\n",
    "        if np.isnan(fz_theta):\n",
    "            print(\"Compute PSD & Ratios: Warning: Cannot compute Alpha/Theta ratio due to missing or zero 'Fz Theta'.\")\n",
    "\n",
    "        # We want the average across all channels for ratio2\n",
    "        selected_channels = ['Fz', 'Cz', 'Pz']\n",
    "        available_channels = psd_df['Channel'].values\n",
    "        missing_channels = [ch for ch in selected_channels if ch not in available_channels]\n",
    "        if missing_channels:\n",
    "            print(f\"Compute PSD & Ratios: Warning: Missing channels in data: {missing_channels}\")\n",
    "\n",
    "        # Filter for available channels\n",
    "        psd_filtered = psd_df[psd_df['Channel'].isin(selected_channels)]\n",
    "\n",
    "        # Calculate mean power across selected channels\n",
    "        mean_alpha = psd_filtered['psd_Alpha'].mean()\n",
    "        mean_theta = psd_filtered['psd_Theta'].mean()\n",
    "        mean_beta = psd_filtered['psd_Beta'].mean()\n",
    "\n",
    "        # Handle NaN values\n",
    "        if np.isnan(mean_alpha) or np.isnan(mean_theta) or np.isnan(mean_beta):\n",
    "            print(\"Compute PSD & Ratios: Warning: Missing values in the selected channels. Ratios may be inaccurate.\")\n",
    "            beta_combined_ratio = np.nan\n",
    "        else:\n",
    "            beta_combined_ratio = mean_beta / (mean_alpha + mean_theta)\n",
    "\n",
    "        # Add PID and save ratio results\n",
    "        ratio_results.append({\n",
    "            'Participant ID': participant,\n",
    "            'Pz Alpha': pz_alpha,\n",
    "            'Fz Theta': fz_theta,\n",
    "            'Pz Alpha / Fz Theta': alpha_theta_ratio,\n",
    "            'Beta / (Alpha + Theta)': beta_combined_ratio,\n",
    "        })\n",
    "\n",
    "        ratio_df = pd.DataFrame(ratio_results)\n",
    "        ratio_df.to_csv(f'{output_loc}/{cond}_ratios_analysis_{phase}.csv', index=False)\n",
    "        print(f\"Compute PSD & Ratios: Ratios analysis saved to '{output_loc}/{cond}_ratios_analysis_{phase}.csv'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PSD or ratio computation: {e}\")\n",
    "\n",
    "    # Plot an example PSD (of the last channel processed)\n",
    "    # psd_values, freqs = mne.time_frequency.psd_array_welch(psd_data, sfreq=raw.info['sfreq'], fmin=0.1, fmax=30.0)\n",
    "    # plt.plot(freqs, psd_values[0])\n",
    "    # plt.title('PSD of Fz')\n",
    "    # plt.xlabel('Frequency (Hz)')\n",
    "    # plt.ylabel('Power (uV^2/Hz)')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_gamma_table(epochs, condition_name, participant, cond):\n",
    "    \"\"\"\n",
    "    Computes the gamma power (20-100 Hz) for each epoch and channel,\n",
    "    applies a log–transform (with safeguard for non–positive values),\n",
    "    and then z–transforms (z–scores) the log–powers across channels.\n",
    "    Saves a per-epoch CSV file and another CSV file with the average (across epochs)\n",
    "    for each channel.\n",
    "\n",
    "    Parameters:\n",
    "        epochs (mne.Epochs): The epochs object for the given condition.\n",
    "        condition_name (str): Either \"train\" or \"test\".\n",
    "        participant (str): Participant identifier used for output file path.\n",
    "        cond (str): Additional condition identifier for the output file name.\n",
    "    \"\"\"\n",
    "    if epochs is None:\n",
    "        print(f\"{condition_name.capitalize()} epochs not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nFinalizing gamma table for {condition_name}...\")\n",
    "\n",
    "    gamma_low, gamma_high = 20, 100\n",
    "    sfreq = epochs.info['sfreq']\n",
    "    results = []\n",
    "\n",
    "    # Process each epoch\n",
    "    for epoch_idx, epoch in enumerate(epochs.get_data()):\n",
    "        log_gamma_power = []\n",
    "        # Process each channel in the epoch\n",
    "        for ch_idx, channel_name in enumerate(epochs.ch_names):\n",
    "            psd, freqs = mne.time_frequency.psd_array_welch(\n",
    "                epoch[ch_idx],\n",
    "                sfreq=sfreq,\n",
    "                fmin=gamma_low,\n",
    "                fmax=gamma_high,\n",
    "                n_per_seg=128\n",
    "            )\n",
    "            power = psd.mean()\n",
    "            if power <= 0:\n",
    "                power = 1e-10  # safeguard against non-positive power\n",
    "            log_power = np.log(power)\n",
    "            log_gamma_power.append(log_power)\n",
    "\n",
    "        log_gamma_power = np.array(log_gamma_power)\n",
    "        mean_val = log_gamma_power.mean()\n",
    "        std_val = log_gamma_power.std()\n",
    "\n",
    "        if std_val == 0:\n",
    "            z_gamma = np.zeros_like(log_gamma_power)\n",
    "        else:\n",
    "            z_gamma = (log_gamma_power - mean_val) / std_val\n",
    "\n",
    "        # Build result dictionary for this epoch\n",
    "        epoch_result = {\"Epoch\": epoch_idx + 1}\n",
    "        for ch_idx, channel_name in enumerate(epochs.ch_names):\n",
    "            epoch_result[channel_name] = z_gamma[ch_idx]\n",
    "        results.append(epoch_result)\n",
    "\n",
    "    # Save per-epoch table\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file = f\"Results/{participant}/{cond}_log_z_gamma_power_{condition_name}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Finalized gamma table saved to '{output_file}'.\")\n",
    "\n",
    "    # Average across epochs for each channel\n",
    "    avg_gamma = df.drop(columns=[\"Epoch\"]).mean()\n",
    "    avg_df = pd.DataFrame(avg_gamma).transpose()\n",
    "    avg_df.insert(0, \"Condition\", condition_name)\n",
    "    avg_output_file = f\"Results/{participant}/{cond}_avg_log_z_gamma_power_{condition_name}.csv\"\n",
    "    avg_df.to_csv(avg_output_file, index=False)\n",
    "    print(f\"Averaged gamma power (log_z) saved to '{avg_output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fft(epochs, condition_name, output_loc, cond):\n",
    "    print(f\"\\nComputing FFT for {condition_name}...\")\n",
    "    fft_results = []\n",
    "    sfreq = epochs.info['sfreq']\n",
    "    n_fft = int(sfreq)  # One second worth of samples\n",
    "    for epoch_idx, epoch_data in enumerate(epochs.get_data()):\n",
    "        for ch_idx, channel_name in enumerate(epochs.ch_names):\n",
    "            fft_vals = np.fft.rfft(epoch_data[ch_idx], n=n_fft)\n",
    "            freqs = np.fft.rfftfreq(n_fft, d=1/sfreq)\n",
    "            fft_results.append({\n",
    "                'Epoch': epoch_idx + 1,\n",
    "                'Channel': channel_name,\n",
    "                'Frequencies': freqs.tolist(),\n",
    "                'FFT_Values': np.abs(fft_vals).tolist()\n",
    "            })\n",
    "    fft_df = pd.DataFrame(fft_results)\n",
    "    output_file = f\"{output_loc}/{cond}_fft_{condition_name}.csv\"\n",
    "    fft_df.to_csv(output_file, index=False)\n",
    "    print(f\"FFT results saved to '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_band_averages(epochs, condition_name, regions, output_loc, cond):\n",
    "    print(f\"\\nComputing band averages for {condition_name}...\")\n",
    "    bands = {\n",
    "        'Delta': (0.5, 4),\n",
    "        'Theta': (4, 7),\n",
    "        'Alpha': (8, 13),\n",
    "        'Beta': (14, 20),\n",
    "        'Gamma': (20, 100)\n",
    "    }\n",
    "    results = []\n",
    "    try:\n",
    "        psd = epochs.compute_psd(method='multitaper', fmin=1, fmax=100)\n",
    "        psd_data = psd.get_data()  # shape: (epochs, channels, frequencies)\n",
    "        freqs = psd.freqs\n",
    "        for region, channels in regions.items():\n",
    "            for band, (fmin, fmax) in bands.items():\n",
    "                selected_channels = [ch for ch in channels if ch in epochs.ch_names]\n",
    "                if not selected_channels:\n",
    "                    print(f\"No channels found for region {region}. Skipping.\")\n",
    "                    continue\n",
    "                channel_indices = [epochs.ch_names.index(ch) for ch in selected_channels]\n",
    "                freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "                if not freq_mask.any():\n",
    "                    print(f\"No frequencies found for band {band} in region {region}. Skipping.\")\n",
    "                    continue\n",
    "                band_power = psd_data[:, channel_indices, :][:, :, freq_mask].mean(axis=(0, 2))\n",
    "                results.append({\n",
    "                    'Region': region,\n",
    "                    'Band': band,\n",
    "                    'Average_Power': band_power.mean(),\n",
    "                })\n",
    "        df = pd.DataFrame(results)\n",
    "        output_file = f\"{output_loc}/{cond}_band_averages_{condition_name}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Band averages saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during band average computation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Creating Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ah never even got to this initially - would be nice to revisit\n",
    "\n",
    "# compute_training_test_ratios()\n",
    "# save_participant_data()\n",
    "# compare_conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_training_test_ratios():\n",
    "    print(\"\\nComputing dynamic training and test ratios...\")\n",
    "    try:\n",
    "        psd_df = pd.read_csv(f\"GammaResults/{participant}/psd_results.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'psd_results.csv' file not found.\")\n",
    "        return\n",
    "    required_columns = ['Channel', 'psd_alpha', 'psd_theta']\n",
    "    if not all(col in psd_df.columns for col in required_columns):\n",
    "        print(f\"Error: Missing required columns in 'psd_results.csv'. Expected columns: {required_columns}\")\n",
    "        return\n",
    "    try:\n",
    "        pz_alpha = psd_df.loc[psd_df['Channel'] == 'Fz', 'psd_alpha'].mean()\n",
    "        fz_theta = psd_df.loc[psd_df['Channel'] == 'Fp1', 'psd_theta'].mean()\n",
    "        training_ratio1 = pz_alpha / fz_theta if fz_theta else np.nan\n",
    "        training_ratio2 = (pz_alpha + fz_theta) / 2 if pz_alpha and fz_theta else np.nan\n",
    "        test_ratio1 = training_ratio1 * 0.9 if training_ratio1 else np.nan\n",
    "        test_ratio2 = training_ratio2 * 0.9 if training_ratio2 else np.nan\n",
    "        ratios = {\n",
    "            'Participant ID': ['Subject_02'],\n",
    "            'Cond1_Training_ratio1': [training_ratio1],\n",
    "            'Cond1_Training_ratio2': [training_ratio2],\n",
    "            'Cond2_Training_ratio1': [test_ratio1],\n",
    "            'Cond2_Training_ratio2': [test_ratio2],\n",
    "        }\n",
    "        output_file = f\"GammaResults/{participant}/training_test_ratios.csv\"\n",
    "        pd.DataFrame(ratios).to_csv(output_file, index=False)\n",
    "        print(f\"Dynamic training and test ratios saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ratio computation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_participant_data():\n",
    "    print(\"\\nConsolidating participant-level data...\")\n",
    "    required_files = [\n",
    "        f\"GammaResults/{participant}/ratios_analysis.csv\",\n",
    "        f\"GammaResults/{participant}/Condition1_band_averages.csv\",\n",
    "        f\"GammaResults/{participant}/Condition2_band_averages.csv\",\n",
    "        f\"GammaResults/{participant}/band_power_Condition1.csv\",\n",
    "        f\"GammaResults/{participant}/band_power_Condition2.csv\",\n",
    "    ]\n",
    "    missing_files = [file for file in required_files if not os.path.exists(file)]\n",
    "    if missing_files:\n",
    "        print(f\"Missing files required for participant data consolidation: {missing_files}\")\n",
    "        return\n",
    "    try:\n",
    "        ratios_df = pd.read_csv(f\"GammaResults/{participant}/ratios_analysis.csv\")\n",
    "        band_averages_df = pd.concat(\n",
    "            [pd.read_csv(f\"GammaResults/{participant}/{condition}_band_averages.csv\")\n",
    "             for condition in [\"Condition1\", \"Condition2\"]],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        band_power_df = pd.concat(\n",
    "            [pd.read_csv(f\"GammaResults/{participant}/band_power_{condition}.csv\")\n",
    "             for condition in [\"Condition1\", \"Condition2\"]],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        for df_name, df in [(\"Ratios\", ratios_df), (\"Band Averages\", band_averages_df), (\"Band Power\", band_power_df)]:\n",
    "            if \"Participant ID\" not in df.columns:\n",
    "                print(f\"Error: 'Participant ID' column missing in {df_name} data.\")\n",
    "                return\n",
    "        participant_data = pd.merge(ratios_df, band_averages_df, on=\"Participant ID\", how=\"outer\")\n",
    "        participant_data = pd.merge(participant_data, band_power_df, on=\"Participant ID\", how=\"outer\")\n",
    "        if participant_data.empty:\n",
    "            print(\"Error: Consolidated participant data is empty. No output saved.\")\n",
    "            return\n",
    "        output_file = f\"GammaResults/{participant}/participant_data.csv\"\n",
    "        participant_data.to_csv(output_file, index=False)\n",
    "        print(f\"Participant-level data saved to '{output_file}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error while saving participant data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_conditions(output_loc):\n",
    "    print(\"\\nPerforming Statistical Analysis...\")\n",
    "    try:\n",
    "        df = pd.read_csv(f\"{output_loc}/training_test_ratios.csv\")\n",
    "        required_columns = ['Cond1_Training_ratio1', 'Cond2_Training_ratio1']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            raise KeyError(f\"Missing required columns: {required_columns}\")\n",
    "        cond1 = df['Cond1_Training_ratio1']\n",
    "        cond2 = df['Cond2_Training_ratio1']\n",
    "        t_stat, p_val = ttest_ind(cond1, cond2)\n",
    "        stats = {'t_stat': [t_stat], 'p_value': [p_val]}\n",
    "        stats_df = pd.DataFrame(stats)\n",
    "        stats_df.to_csv(f\"{output_loc}/t_test_results.csv\", index=False)\n",
    "        print(f\"T-test results saved to '{output_loc}/t_test_results.csv': t-stat={t_stat}, p-val={p_val}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError during statistical analysis: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during statistical analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step 8: Process Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params: set_file, marker_csv, channel_mat, fixed_channels, prestim, poststim, baseline_window\n",
    "\n",
    "# sfreq = None\n",
    "# condition_timestamps = {}\n",
    "# trl = []\n",
    "# participant_ratios = []\n",
    "# epochs_train = None\n",
    "# epochs_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline(set_file, marker_csv, channel_mat, fixed_channels, channel_names, prestim, poststim, baseline_window, participant, cond, cond_start, cond_end, output_loc):\n",
    "    print(\"Starting processing pipeline...\\n\")\n",
    "\n",
    "    print(\"\\nStep 1: Global Variables Initialized\")\n",
    "\n",
    "    print(\"\\nStep 2: Data Loading & Channel Information\")\n",
    "\n",
    "    # Load marker CSV and channel info\n",
    "    marker_df = pd.read_csv(marker_csv)\n",
    "    channel_info = load_channel_info(channel_mat, channel_names)\n",
    "    print(f'Process_Pipeline: Channel Info: {channel_info}')\n",
    "\n",
    "    # Convert prestim and poststim from ms to seconds if needed\n",
    "    prestim = prestim / 1000 # Why divide by 1000?\n",
    "    poststim = poststim / 1000\n",
    "\n",
    "    # Task: May not have to pass fixed_channels\n",
    "    # raw = load_set(set_file, channel_info, fixed_channels)\n",
    "    raw = load_set(set_file, channel_info)\n",
    "\n",
    "    print(\"\\nStep 3: Event Extraction\")\n",
    "\n",
    "    # Extract training and test segments based on marker events\n",
    "    raw_training, raw_test = extract_event_windows(raw, marker_df, cond_start, cond_end, output_loc, cond)\n",
    "\n",
    "    # Process training data\n",
    "    if raw_training:\n",
    "        print(\"\\n***********TRAIN***********\")\n",
    "        print(\"\\nStep 4: Preprocessing Functions\")\n",
    "        raw_training = apply_baseline_correction(raw_training, marker_df, \"train\", output_loc, cond)\n",
    "        raw_training, ica_training = apply_ica(raw_training, fixed_channels, \"train\", output_loc, cond)\n",
    "        raw_training = apply_autoreject(raw_training)\n",
    "        print(\"\\nStep 5: Epoching\")\n",
    "        epochs_training = epoch_data(raw_training, \"train\", output_loc, cond, duration=1.0, overlap=0.0)\n",
    "        if epochs_training:\n",
    "            print(\"\\nStep 6: Analysis Functions\")\n",
    "            compute_and_save_band_power(epochs_training, \"train\", output_loc, cond)\n",
    "            compute_psd_and_ratios(raw_training, \"train\", participant, output_loc, cond)\n",
    "            \n",
    "            finalize_gamma_table(epochs_training, \"train\", participant, cond)\n",
    "            \n",
    "            compute_fft(epochs_training, \"train\", output_loc, cond)\n",
    "            compute_band_averages(epochs_training, \"train\", regions, output_loc, cond)\n",
    "\n",
    "    # Process test data\n",
    "    if raw_test:\n",
    "        print(\"\\n***********TEST***********\")\n",
    "        print(\"\\nStep 4: Preprocessing Functions\")\n",
    "        raw_test = apply_baseline_correction(raw_test, marker_df, \"test\", output_loc, cond)\n",
    "        raw_test, ica_test = apply_ica(raw_test, fixed_channels, \"test\", output_loc, cond)\n",
    "        raw_test = apply_autoreject(raw_test)\n",
    "        print(\"\\nStep 5: Epoching\")\n",
    "        epochs_test = epoch_data(raw_test, \"test\", output_loc, cond, duration=1.0, overlap=0.0)\n",
    "        if epochs_test:\n",
    "            print(\"\\nStep 6: Analysis Functions\")\n",
    "            compute_and_save_band_power(epochs_test, \"test\", output_loc, cond)\n",
    "            compute_psd_and_ratios(raw_test, \"test\", participant, output_loc, cond)\n",
    "            \n",
    "            finalize_gamma_table(epochs_test, \"test\", participant, cond)\n",
    "            \n",
    "            compute_fft(epochs_test, \"test\", output_loc, cond)\n",
    "            compute_band_averages(epochs_test, \"test\", regions, output_loc, cond)\n",
    "\n",
    "    # # Additional computations\n",
    "    # compute_training_test_ratios()\n",
    "    # save_participant_data()\n",
    "    # compare_conditions(output_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Obtain .XDF File\n",
      "Starting processing pipeline...\n",
      "\n",
      "\n",
      "Step 1: Global Variables Initialized\n",
      "\n",
      "Step 2: Data Loading & Channel Information\n",
      "Step 2.1: Loading channel.mat file for channel names...\n",
      "Load Channel Info: Channel.mat Data: {'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Jan 15 12:32:34 2025', '__version__': '1.0', '__globals__': [], 'Channel': array([[(array(['Fz  ', 'Cz  ', 'Pz  ', 'Acc1', 'Acc2', 'Acc3'], dtype='<U4'), array([[0.1, 0. , 0. ],\n",
      "               [0.2, 0.1, 0. ],\n",
      "               [0.3, 0.2, 0. ],\n",
      "               [0. , 0.1, 0.2],\n",
      "               [0. , 0.2, 0.3],\n",
      "               [0. , 0.3, 0.4]]))                                                                     ]],\n",
      "      dtype=[('Name', 'O'), ('Loc', 'O')])}\n",
      "Load Channel Info: Hardcoded Channel Names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Process_Pipeline: Channel Info: {'names': ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3'], 'locs': array([[0.1, 0.2, 0.3],\n",
      "       [0. , 0.1, 0.2],\n",
      "       [0. , 0. , 0. ]])}\n",
      "Step 2.2: Loading .xdf file...\n",
      "Load Set: sfreq: 250\n",
      "Load Set: Channel names: ['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC5', 'FC1', 'C3', 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
      "Load Set: Original hardcoded channel names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Load Set: Dropping channels: ['F7', 'F9', 'FC5', 'FC1', 'C3']\n",
      "Step 2.2.1: Downsampling to 250 Hz...\n",
      "Step 2.2.2: Applying bandpass filter (0.1 Hz to 30 Hz)...\n",
      "Load Set: EEG data successfully loaded and preprocessed.\n",
      "\n",
      "Step 3: Event Extraction\n",
      "\n",
      "Step 3.1: Extracting event windows for 'training' and 'test' periods...\n",
      "Extract Event Windows: LSL Values: 0      30.0\n",
      "1      40.0\n",
      "2      60.0\n",
      "3      20.0\n",
      "4      50.0\n",
      "       ... \n",
      "325     4.0\n",
      "326     4.0\n",
      "327     4.0\n",
      "328     4.0\n",
      "329    14.0\n",
      "Name: value, Length: 330, dtype: float64\n",
      "Extract Event Windows: Indices of First Condition Start: 42.0\n",
      "Extract Event Windows: Actual training start: 2173.514\n",
      "Extract Event Windows: Actual training end: 2521.492\n",
      "Extract Event Windows: Training data saved to 'Results/P01/fif/LL_training_data.fif'\n",
      "Extract Event Windows: Test data saved to 'Results/P01/fif/LL_test_data.fif'\n",
      "Extract Event Windows: Training data duration: 347.97800000000007 seconds\n",
      "Extract Event Windows: Test data duration: 34.13799999999992 seconds\n",
      "Extract Event Windows: Condition timestamps saved to 'Results/P01/LL_condition_timestamps.csv'.\n",
      "Extract Event Windows: Trials saved to 'Results/P01/LL_trials.csv'.\n",
      "\n",
      "***********TRAIN***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/LL_D_bc_train.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/LL_D_cleaned_train.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jalynn\\anaconda3\\envs\\Universal_EEG_Analyzer\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 77000\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=308.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 308 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/LL_epoch_trl_train.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for train...\n",
      "Band power results saved to 'Results/P01/LL_band_power_train.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/LL_psd_results_train.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/LL_ratios_analysis_train.csv'.\n",
      "\n",
      "Finalizing gamma table for train...\n",
      "Finalized gamma table saved to 'Results/P01/LL_log_z_gamma_power_train.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/LL_avg_log_z_gamma_power_train.csv'.\n",
      "\n",
      "Computing FFT for train...\n",
      "FFT results saved to 'Results/P01/LL_fft_train.csv'.\n",
      "\n",
      "Computing band averages for train...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/LL_band_averages_train.csv'.\n",
      "\n",
      "***********TEST***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/LL_D_bc_test.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/LL_D_cleaned_test.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 5500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=22.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 22 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/LL_epoch_trl_test.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for test...\n",
      "Band power results saved to 'Results/P01/LL_band_power_test.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/LL_psd_results_test.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/LL_ratios_analysis_test.csv'.\n",
      "\n",
      "Finalizing gamma table for test...\n",
      "Finalized gamma table saved to 'Results/P01/LL_log_z_gamma_power_test.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/LL_avg_log_z_gamma_power_test.csv'.\n",
      "\n",
      "Computing FFT for test...\n",
      "FFT results saved to 'Results/P01/LL_fft_test.csv'.\n",
      "\n",
      "Computing band averages for test...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/LL_band_averages_test.csv'.\n",
      "Step 0: Obtain .XDF File\n",
      "Starting processing pipeline...\n",
      "\n",
      "\n",
      "Step 1: Global Variables Initialized\n",
      "\n",
      "Step 2: Data Loading & Channel Information\n",
      "Step 2.1: Loading channel.mat file for channel names...\n",
      "Load Channel Info: Channel.mat Data: {'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Jan 15 12:32:34 2025', '__version__': '1.0', '__globals__': [], 'Channel': array([[(array(['Fz  ', 'Cz  ', 'Pz  ', 'Acc1', 'Acc2', 'Acc3'], dtype='<U4'), array([[0.1, 0. , 0. ],\n",
      "               [0.2, 0.1, 0. ],\n",
      "               [0.3, 0.2, 0. ],\n",
      "               [0. , 0.1, 0.2],\n",
      "               [0. , 0.2, 0.3],\n",
      "               [0. , 0.3, 0.4]]))                                                                     ]],\n",
      "      dtype=[('Name', 'O'), ('Loc', 'O')])}\n",
      "Load Channel Info: Hardcoded Channel Names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Process_Pipeline: Channel Info: {'names': ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3'], 'locs': array([[0.1, 0.2, 0.3],\n",
      "       [0. , 0.1, 0.2],\n",
      "       [0. , 0. , 0. ]])}\n",
      "Step 2.2: Loading .xdf file...\n",
      "Load Set: sfreq: 250\n",
      "Load Set: Channel names: ['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC5', 'FC1', 'C3', 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
      "Load Set: Original hardcoded channel names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Load Set: Dropping channels: ['F7', 'F9', 'FC5', 'FC1', 'C3']\n",
      "Step 2.2.1: Downsampling to 250 Hz...\n",
      "Step 2.2.2: Applying bandpass filter (0.1 Hz to 30 Hz)...\n",
      "Load Set: EEG data successfully loaded and preprocessed.\n",
      "\n",
      "Step 3: Event Extraction\n",
      "\n",
      "Step 3.1: Extracting event windows for 'training' and 'test' periods...\n",
      "Extract Event Windows: LSL Values: 0      30.0\n",
      "1      40.0\n",
      "2      60.0\n",
      "3      20.0\n",
      "4      50.0\n",
      "       ... \n",
      "325     4.0\n",
      "326     4.0\n",
      "327     4.0\n",
      "328     4.0\n",
      "329    14.0\n",
      "Name: value, Length: 330, dtype: float64\n",
      "Extract Event Windows: Indices of First Condition Start: 43.0\n",
      "Extract Event Windows: Actual training start: 2735.388\n",
      "Extract Event Windows: Actual training end: 3087.074\n",
      "Extract Event Windows: Training data saved to 'Results/P01/fif/LH_training_data.fif'\n",
      "Extract Event Windows: Test data saved to 'Results/P01/fif/LH_test_data.fif'\n",
      "Extract Event Windows: Training data duration: 351.68600000000015 seconds\n",
      "Extract Event Windows: Test data duration: 43.75 seconds\n",
      "Extract Event Windows: Condition timestamps saved to 'Results/P01/LH_condition_timestamps.csv'.\n",
      "Extract Event Windows: Trials saved to 'Results/P01/LH_trials.csv'.\n",
      "\n",
      "***********TRAIN***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/LH_D_bc_train.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/LH_D_cleaned_train.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 63500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=254.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 254 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/LH_epoch_trl_train.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for train...\n",
      "Band power results saved to 'Results/P01/LH_band_power_train.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/LH_psd_results_train.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/LH_ratios_analysis_train.csv'.\n",
      "\n",
      "Finalizing gamma table for train...\n",
      "Finalized gamma table saved to 'Results/P01/LH_log_z_gamma_power_train.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/LH_avg_log_z_gamma_power_train.csv'.\n",
      "\n",
      "Computing FFT for train...\n",
      "FFT results saved to 'Results/P01/LH_fft_train.csv'.\n",
      "\n",
      "Computing band averages for train...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/LH_band_averages_train.csv'.\n",
      "\n",
      "***********TEST***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/LH_D_bc_test.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/LH_D_cleaned_test.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 9500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=38.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 38 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/LH_epoch_trl_test.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for test...\n",
      "Band power results saved to 'Results/P01/LH_band_power_test.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/LH_psd_results_test.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/LH_ratios_analysis_test.csv'.\n",
      "\n",
      "Finalizing gamma table for test...\n",
      "Finalized gamma table saved to 'Results/P01/LH_log_z_gamma_power_test.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/LH_avg_log_z_gamma_power_test.csv'.\n",
      "\n",
      "Computing FFT for test...\n",
      "FFT results saved to 'Results/P01/LH_fft_test.csv'.\n",
      "\n",
      "Computing band averages for test...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/LH_band_averages_test.csv'.\n",
      "Step 0: Obtain .XDF File\n",
      "Starting processing pipeline...\n",
      "\n",
      "\n",
      "Step 1: Global Variables Initialized\n",
      "\n",
      "Step 2: Data Loading & Channel Information\n",
      "Step 2.1: Loading channel.mat file for channel names...\n",
      "Load Channel Info: Channel.mat Data: {'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Jan 15 12:32:34 2025', '__version__': '1.0', '__globals__': [], 'Channel': array([[(array(['Fz  ', 'Cz  ', 'Pz  ', 'Acc1', 'Acc2', 'Acc3'], dtype='<U4'), array([[0.1, 0. , 0. ],\n",
      "               [0.2, 0.1, 0. ],\n",
      "               [0.3, 0.2, 0. ],\n",
      "               [0. , 0.1, 0.2],\n",
      "               [0. , 0.2, 0.3],\n",
      "               [0. , 0.3, 0.4]]))                                                                     ]],\n",
      "      dtype=[('Name', 'O'), ('Loc', 'O')])}\n",
      "Load Channel Info: Hardcoded Channel Names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Process_Pipeline: Channel Info: {'names': ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3'], 'locs': array([[0.1, 0.2, 0.3],\n",
      "       [0. , 0.1, 0.2],\n",
      "       [0. , 0. , 0. ]])}\n",
      "Step 2.2: Loading .xdf file...\n",
      "Load Set: sfreq: 250\n",
      "Load Set: Channel names: ['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC5', 'FC1', 'C3', 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
      "Load Set: Original hardcoded channel names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Load Set: Dropping channels: ['F7', 'F9', 'FC5', 'FC1', 'C3']\n",
      "Step 2.2.1: Downsampling to 250 Hz...\n",
      "Step 2.2.2: Applying bandpass filter (0.1 Hz to 30 Hz)...\n",
      "Load Set: EEG data successfully loaded and preprocessed.\n",
      "\n",
      "Step 3: Event Extraction\n",
      "\n",
      "Step 3.1: Extracting event windows for 'training' and 'test' periods...\n",
      "Extract Event Windows: LSL Values: 0      30.0\n",
      "1      40.0\n",
      "2      60.0\n",
      "3      20.0\n",
      "4      50.0\n",
      "       ... \n",
      "325     4.0\n",
      "326     4.0\n",
      "327     4.0\n",
      "328     4.0\n",
      "329    14.0\n",
      "Name: value, Length: 330, dtype: float64\n",
      "Extract Event Windows: Indices of First Condition Start: 44.0\n",
      "Extract Event Windows: Actual training start: 3311.916\n",
      "Extract Event Windows: Actual training end: 3701.602\n",
      "Extract Event Windows: Training data saved to 'Results/P01/fif/HL_training_data.fif'\n",
      "Extract Event Windows: Test data saved to 'Results/P01/fif/HL_test_data.fif'\n",
      "Extract Event Windows: Training data duration: 389.6859999999997 seconds\n",
      "Extract Event Windows: Test data duration: 43.764000000000124 seconds\n",
      "Extract Event Windows: Condition timestamps saved to 'Results/P01/HL_condition_timestamps.csv'.\n",
      "Extract Event Windows: Trials saved to 'Results/P01/HL_trials.csv'.\n",
      "\n",
      "***********TRAIN***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/HL_D_bc_train.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/HL_D_cleaned_train.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 92500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=370.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 370 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/HL_epoch_trl_train.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for train...\n",
      "Band power results saved to 'Results/P01/HL_band_power_train.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/HL_psd_results_train.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/HL_ratios_analysis_train.csv'.\n",
      "\n",
      "Finalizing gamma table for train...\n",
      "Finalized gamma table saved to 'Results/P01/HL_log_z_gamma_power_train.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/HL_avg_log_z_gamma_power_train.csv'.\n",
      "\n",
      "Computing FFT for train...\n",
      "FFT results saved to 'Results/P01/HL_fft_train.csv'.\n",
      "\n",
      "Computing band averages for train...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/HL_band_averages_train.csv'.\n",
      "\n",
      "***********TEST***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/HL_D_bc_test.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/HL_D_cleaned_test.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "No bad epochs were found for your data. Returning a copy of the data you wanted to clean. Interpolation may have been done.\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 10500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=42.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 42 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/HL_epoch_trl_test.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for test...\n",
      "Band power results saved to 'Results/P01/HL_band_power_test.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/HL_psd_results_test.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/HL_ratios_analysis_test.csv'.\n",
      "\n",
      "Finalizing gamma table for test...\n",
      "Finalized gamma table saved to 'Results/P01/HL_log_z_gamma_power_test.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/HL_avg_log_z_gamma_power_test.csv'.\n",
      "\n",
      "Computing FFT for test...\n",
      "FFT results saved to 'Results/P01/HL_fft_test.csv'.\n",
      "\n",
      "Computing band averages for test...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/HL_band_averages_test.csv'.\n",
      "Step 0: Obtain .XDF File\n",
      "Starting processing pipeline...\n",
      "\n",
      "\n",
      "Step 1: Global Variables Initialized\n",
      "\n",
      "Step 2: Data Loading & Channel Information\n",
      "Step 2.1: Loading channel.mat file for channel names...\n",
      "Load Channel Info: Channel.mat Data: {'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Wed Jan 15 12:32:34 2025', '__version__': '1.0', '__globals__': [], 'Channel': array([[(array(['Fz  ', 'Cz  ', 'Pz  ', 'Acc1', 'Acc2', 'Acc3'], dtype='<U4'), array([[0.1, 0. , 0. ],\n",
      "               [0.2, 0.1, 0. ],\n",
      "               [0.3, 0.2, 0. ],\n",
      "               [0. , 0.1, 0.2],\n",
      "               [0. , 0.2, 0.3],\n",
      "               [0. , 0.3, 0.4]]))                                                                     ]],\n",
      "      dtype=[('Name', 'O'), ('Loc', 'O')])}\n",
      "Load Channel Info: Hardcoded Channel Names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Process_Pipeline: Channel Info: {'names': ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3'], 'locs': array([[0.1, 0.2, 0.3],\n",
      "       [0. , 0.1, 0.2],\n",
      "       [0. , 0. , 0. ]])}\n",
      "Step 2.2: Loading .xdf file...\n",
      "Load Set: sfreq: 250\n",
      "Load Set: Channel names: ['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC5', 'FC1', 'C3', 'ACC_X', 'ACC_Y', 'ACC_Z']\n",
      "Load Set: Original hardcoded channel names: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "Load Set: Dropping channels: ['F7', 'F9', 'FC5', 'FC1', 'C3']\n",
      "Step 2.2.1: Downsampling to 250 Hz...\n",
      "Step 2.2.2: Applying bandpass filter (0.1 Hz to 30 Hz)...\n",
      "Load Set: EEG data successfully loaded and preprocessed.\n",
      "\n",
      "Step 3: Event Extraction\n",
      "\n",
      "Step 3.1: Extracting event windows for 'training' and 'test' periods...\n",
      "Extract Event Windows: LSL Values: 0      30.0\n",
      "1      40.0\n",
      "2      60.0\n",
      "3      20.0\n",
      "4      50.0\n",
      "       ... \n",
      "325     4.0\n",
      "326     4.0\n",
      "327     4.0\n",
      "328     4.0\n",
      "329    14.0\n",
      "Name: value, Length: 330, dtype: float64\n",
      "Extract Event Windows: Indices of First Condition Start: 46.0\n",
      "Extract Event Windows: Actual training start: 1381.798\n",
      "Extract Event Windows: Actual training end: 1909.772\n",
      "Extract Event Windows: Training data saved to 'Results/P01/fif/HH_training_data.fif'\n",
      "Extract Event Windows: Test data saved to 'Results/P01/fif/HH_test_data.fif'\n",
      "Extract Event Windows: Training data duration: 527.9739999999999 seconds\n",
      "Extract Event Windows: Test data duration: 64.84600000000023 seconds\n",
      "Extract Event Windows: Condition timestamps saved to 'Results/P01/HH_condition_timestamps.csv'.\n",
      "Extract Event Windows: Trials saved to 'Results/P01/HH_trials.csv'.\n",
      "\n",
      "***********TRAIN***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/HH_D_bc_train.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/HH_D_cleaned_train.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 127000\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=508.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 508 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/HH_epoch_trl_train.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for train...\n",
      "Band power results saved to 'Results/P01/HH_band_power_train.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/HH_psd_results_train.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/HH_ratios_analysis_train.csv'.\n",
      "\n",
      "Finalizing gamma table for train...\n",
      "Finalized gamma table saved to 'Results/P01/HH_log_z_gamma_power_train.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/HH_avg_log_z_gamma_power_train.csv'.\n",
      "\n",
      "Computing FFT for train...\n",
      "FFT results saved to 'Results/P01/HH_fft_train.csv'.\n",
      "\n",
      "Computing band averages for train...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/HH_band_averages_train.csv'.\n",
      "\n",
      "***********TEST***********\n",
      "\n",
      "Step 4: Preprocessing Functions\n",
      "\n",
      "Step 4.1: Applying baseline correction...\n",
      "Apply Baseline Correction: Detecting rest intervals from markers...\n",
      "Apply Baseline Correction: Using detected rest interval for baseline correction: Start=62, End=63\n",
      "Apply Baseline Correction: Baseline correction using rest interval applied successfully.\n",
      "Saving baseline-corrected file to 'Results/P01/fif/HH_D_bc_test.fif'...\n",
      "\n",
      "Step 4.2: Applying ICA...\n",
      "Apply ICA: EEG Picks: [0 1 2]\n",
      "Apply ICA: Found EOG components: []\n",
      "Apply ICA: ECG artifact detection skipped: Generating an artificial ECG channel can only be done for MEG data.\n",
      "Apply ICA: Excluded components: []\n",
      "Apply ICA: ICA applied with 3 components. Excluded 0 components.\n",
      "Apply ICA: Cleaned EEG file saved as 'Results/P01/fif/HH_D_cleaned_test.fif'.\n",
      "\n",
      "Step 4.3: Applying AutoReject (Repair Only)...\n",
      "Apply AutoReject: Trying consensus=1.0...\n",
      "Running autoreject on ch_type=eeg\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Estimated consensus=1.00 and n_interpolate=2\n",
      "Apply AutoReject: Success with consensus=1.0!\n",
      "Apply AutoReject: Channels after AutoReject: ['Fz', 'Cz', 'Pz', 'Acc1', 'Acc2', 'Acc3']\n",
      "\n",
      "Step 5: Epoching\n",
      "\n",
      "Step 5.1: Epoching data...\n",
      "Epoch Data: Raw n times: 15500\n",
      "Epoch Data: Raw info sfreq: 250.0\n",
      "Epoch Data: Raw data range: Start=0, Stop=62.0, Duration=1.0, Overlap=0.0\n",
      "Epoch Data: Generated 62 fixed-length events.\n",
      "Epoch Data: Epoch trial data saved to 'Results/P01/HH_epoch_trl_test.csv'.\n",
      "\n",
      "Step 6: Analysis Functions\n",
      "\n",
      "Step 6.1: Computing band power for test...\n",
      "Band power results saved to 'Results/P01/HH_band_power_test.csv'.\n",
      "\n",
      "Step 6.2: Computing PSD and ratios for analysis...\n",
      "Compute PSD & Ratios: PSD results saved to 'Results/P01/HH_psd_results_test.csv'.\n",
      "Compute PSD & Ratios: Ratios analysis saved to 'Results/P01/HH_ratios_analysis_test.csv'.\n",
      "\n",
      "Finalizing gamma table for test...\n",
      "Finalized gamma table saved to 'Results/P01/HH_log_z_gamma_power_test.csv'.\n",
      "Averaged gamma power (log_z) saved to 'Results/P01/HH_avg_log_z_gamma_power_test.csv'.\n",
      "\n",
      "Computing FFT for test...\n",
      "FFT results saved to 'Results/P01/HH_fft_test.csv'.\n",
      "\n",
      "Computing band averages for test...\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Central. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Parietal. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "No channels found for region Occipital. Skipping.\n",
      "Band averages saved to 'Results/P01/HH_band_averages_test.csv'.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    for participant in participants:\n",
    "        for cond, params in conditions.items():\n",
    "            \n",
    "            cond_start = params[\"cond_start\"]\n",
    "            cond_end = params[\"cond_end\"]\n",
    "            \n",
    "            print(\"Step 0: Obtain .XDF File\")\n",
    "            \n",
    "            set_file = get_set_file(participant)\n",
    "            marker_csv = f\"Raw/{participant}/{participant}events_data.csv\"\n",
    "            output_loc = f\"Results/{participant}\"\n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            if not os.path.exists(output_loc):\n",
    "                os.makedirs(output_loc)\n",
    "                \n",
    "            other = f\"Results/{participant}/fif\"\n",
    "            \n",
    "            # Ensure the output directory exists\n",
    "            if not os.path.exists(other):\n",
    "                os.makedirs(other)\n",
    "            \n",
    "            process_pipeline(\n",
    "                set_file=set_file,\n",
    "                marker_csv=marker_csv,\n",
    "                channel_mat=\"Raw/new_channel_allocations.mat\",\n",
    "                fixed_channels=['Fp1', 'Fz', 'F3', 'F7', 'F9', 'FC5', 'FC1', 'C3', 'ACC_X', 'ACC_Y', 'ACC_Z'],\n",
    "                channel_names=channel_names,\n",
    "                prestim=0,\n",
    "                poststim=1000,\n",
    "                baseline_window=[2762.35, 2777.488],\n",
    "                participant=participant,\n",
    "                cond=cond,\n",
    "                cond_start=cond_start,\n",
    "                cond_end=cond_end,\n",
    "                output_loc=output_loc\n",
    "            )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Universal_EEG_Analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
